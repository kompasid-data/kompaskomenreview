# -*- coding: utf-8 -*-
"""Kompas_App_Review_Model_and_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylcopZp_g393Cy6BdzII_Os8VV0R8Tx1

#IMPORT LIBRARIES
"""

!pip install langdetect
!pip install transformers
!pip install scipy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import io
import re
import string
import nltk

from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.utils import shuffle
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from sklearn.utils import resample
from sklearn.preprocessing import LabelEncoder
from transformers import pipeline

from langdetect import detect
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax
from transformers import AutoModelForMaskedLM

from wordcloud import WordCloud
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from tqdm import tqdm
from nltk import word_tokenize, pos_tag
from nltk.chunk import ne_chunk

nltk.download('stopwords')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

plt.style.use('ggplot')

from google.colab import files
uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

raw = pd.read_csv(io.StringIO(uploaded['Review.csv'].decode('utf-8')))

"""#RAW DATA VISUALIZATION"""

raw.head()

#RAW SHAPE PRINTING
print(raw.shape)

# Filtering data and preparing for plotting
filtered_ratings = {}
total_reports = {}  # Dictionary to store total reports for each rating

for rating in range(1, 6):
    filtered_ratings[rating] = raw[raw['user_rating'] == rating].copy()
    filtered_ratings[rating]['date'] = pd.to_datetime(filtered_ratings[rating]['date'], errors='coerce')
    filtered_ratings[rating] = filtered_ratings[rating].dropna(subset=['date'])
    filtered_ratings[rating]['year_month'] = filtered_ratings[rating]['date'].dt.to_period('M')
    filtered_ratings[rating] = filtered_ratings[rating].groupby(['year_month', 'user_rating']).size().unstack(fill_value=0)
    total_reports[rating] = filtered_ratings[rating].sum().sum()  # Calculate total reports

# Compute total reports across all ratings
total_reports_all = sum(total_reports.values())

# Calculate percentages for each rating
percentages = {rating: (total_reports[rating] / total_reports_all) * 100 for rating in range(1, 6)}

# Determine common months across all ratings
months_set = set(filtered_ratings[1].index.astype(str))
for rating in range(2, 6):
    months_set &= set(filtered_ratings[rating].index.astype(str))
common_months = sorted(months_set)

# Prepare data for plotting
combined_data = pd.DataFrame(index=common_months)
for rating in range(1, 6):
    combined_data[f'Rating {rating}'] = filtered_ratings[rating].loc[common_months, rating]

# Plotting
plt.figure(figsize=(16, 8))

colors = ['blue', 'green', 'orange', 'purple', 'red']
labels = [f'Rating {i}' for i in range(1, 6)]

combined_data.plot(kind='bar', stacked=True, color=colors, width=0.8, ax=plt.gca())
plt.xlabel('Month')
plt.ylabel('Number of Reports')
plt.title('Number of Reports for Each Rating per Month')
plt.xticks(rotation=45)
plt.legend(labels, title='Ratings', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Filtering data and preparing for plotting
filtered_ratings = {}
total_reports = {}  # Dictionary to store total reports for each rating

for rating in range(1, 6):
    filtered_ratings[rating] = raw[raw['user_rating'] == rating].copy()
    filtered_ratings[rating]['date'] = pd.to_datetime(filtered_ratings[rating]['date'], errors='coerce')
    filtered_ratings[rating] = filtered_ratings[rating].dropna(subset=['date'])
    filtered_ratings[rating]['year_month'] = filtered_ratings[rating]['date'].dt.to_period('M')
    filtered_ratings[rating] = filtered_ratings[rating].groupby(['year_month', 'user_rating']).size().unstack(fill_value=0)
    total_reports[rating] = filtered_ratings[rating].sum().sum()  # Calculate total reports

# Compute total reports across all ratings
total_reports_all = sum(total_reports.values())

# Determine common months across all ratings
months_set = set(filtered_ratings[1].index.astype(str))
for rating in range(2, 6):
    months_set &= set(filtered_ratings[rating].index.astype(str))
common_months = sorted(months_set)

# Prepare data for plotting
combined_data = pd.DataFrame(index=common_months)
for rating in range(1, 6):
    combined_data[f'Rating {rating}'] = filtered_ratings[rating].loc[common_months, rating]

# Plotting
plt.figure(figsize=(16, 8))

colors = ['blue', 'green', 'orange', 'purple', 'red']
labels = [f'Rating {i}' for i in range(1, 6)]

combined_data.plot(kind='line', color=colors, ax=plt.gca())
plt.xlabel('Month')
plt.ylabel('Number of Reports')
plt.title('Number of Reports for Each Rating per Month')
plt.xticks(rotation=45)
plt.legend(labels, title='Ratings', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Calculate the counts and percentages of each rating
rating_counts_raw = raw['user_rating'].value_counts().sort_index()
total_raw = rating_counts_raw.sum()
rating_percentages_raw = (rating_counts_raw / total_raw) * 100

# Print the counts and percentages
print("\nCounts of each rating for data:")
print(rating_counts_raw)
print("\nPercentages of each rating for data without:")
print(rating_percentages_raw)
print("\nTotal number of reports for data:")
total_raw

"""#PREPROCESSING

**FILTER DATASET TO ONLY INCLUDE "ID.KOMPAS.APP"**
"""

# Filter the dataset to include only rows with the package name 'id.kompas.app'
raw_data = raw[raw['package_name'] == 'id.kompas.app']
# Display the filtered dataframe
raw_data.head()

"""**SHOW ONLY REPORTS WITH USER REVIEW FOR ACCURATE ANALYSIS**"""

# Filter the raw_data DataFrame to include only rows with non-empty 'user_review'
raw_data2 = raw_data[raw_data['user_review'].notna() & (raw_data['user_review'] != '')]
print("\nRaw Data non-empty 'user_review' :")
raw_data2

# Filter the raw_data DataFrame to include only rows with empty or null 'user_review'
raw_data_no_review = raw_data[raw_data['user_review'].isna() | (raw_data['user_review'] == '')]

# Calculate the counts and percentages of each rating
rating_counts_no_review = raw_data_no_review['user_rating'].value_counts().sort_index()
total_no_review = rating_counts_no_review.sum()
rating_percentages_no_review = (rating_counts_no_review / total_no_review) * 100

# Print the counts and percentages
print("\nCounts of each rating for data without 'user_review':")
print(rating_counts_no_review)
print("\nPercentages of each rating for data without 'user_review':")
print(rating_percentages_no_review)
print("\nTotal number of reports for data without 'user_review':")
total_no_review

#Find values for batch_uuid etc to see if column can be deleted
num_batch_uuid = raw_data2['batch_uuid'].nunique()
num_unique_id = raw_data2['unique_id'].nunique()
num_last_update = raw_data2['last_update'].nunique()
print(num_batch_uuid, num_unique_id, num_last_update)

"""**Select only necessary columns for analysis**"""

#Select only necessary columns for analysis
selected_columns = ['hd', 'date',   'user_rating', 'user_review']
selected_data = raw_data2[selected_columns]

# Display the selected data
selected_data

"""**#Remove emoji only comments**"""

# Function to remove emojis from text
def remove_emojis(text):
    return re.sub(r'[^\w\s,]', '', text)

# Function to count emojis in text
def count_emojis(text):
    return len(re.findall(r'[^\w\s,]', text))

# Store the original DataFrame
original_data = selected_data.copy()

# Count total emojis in the original data
original_emoji_count = original_data['user_review'].apply(count_emojis).sum()

# Apply the function to the user_review column using .loc
selected_data.loc[:, 'user_review'] = selected_data['user_review'].apply(remove_emojis)

# Count remaining emojis in the processed data
remaining_emoji_count = selected_data['user_review'].apply(count_emojis).sum()

# Calculate the number of removed emojis
removed_emoji_count = original_emoji_count - remaining_emoji_count

# Identify rows that will be removed
removed_values = original_data[original_data['user_review'].str.strip() == '']['user_review'].tolist()
removed_values += original_data.loc[original_data.index.difference(selected_data.index), 'user_review'].tolist()

# Remove empty rows
selected_data = selected_data[selected_data['user_review'].str.strip() != '']

# Show the updated DataFrame
print("Updated DataFrame:")
selected_data

# Show the total number of removed emojis
print("\nTotal number of removed emojis:", removed_emoji_count)

# Group by user_rating and user_review and count the occurrences
grouped = selected_data.groupby(['user_rating', 'user_review']).size().reset_index(name='counts')

# Filter the groups where the count is greater than 1
common_rows = grouped[grouped['counts'] > 1]

# Display common rows
print("Common Rows:")
print(common_rows)

# Merge to find the rows to remove
rows_to_remove = pd.merge(selected_data, common_rows, on=['user_rating', 'user_review'], how='inner')

# Remove these rows from the selected_data DataFrame
selected_data = selected_data[~selected_data.index.isin(rows_to_remove.index)]

# Display the cleaned DataFrame
print("Cleaned DataFrame:")
selected_data

# Filter data for each user_rating value from 1 to 5 separately
filtered_ratings = {}
total_reports = {}  # Dictionary to store total reports for each rating

for rating in range(1, 6):
    filtered_ratings[rating] = selected_data[selected_data['user_rating'] == rating].copy()
    filtered_ratings[rating]['date'] = pd.to_datetime(filtered_ratings[rating]['date'], errors='coerce')
    filtered_ratings[rating] = filtered_ratings[rating].dropna(subset=['date'])
    filtered_ratings[rating]['year_month'] = filtered_ratings[rating]['date'].dt.to_period('M')
    filtered_ratings[rating] = filtered_ratings[rating].groupby(['year_month', 'user_rating']).size().unstack(fill_value=0)
    total_reports[rating] = filtered_ratings[rating].sum().sum()  # Calculate total reports

# Compute total reports across all ratings
total_reports_all = sum(total_reports.values())

# Determine common months across all ratings
months_set = set(filtered_ratings[1].index.astype(str))
for rating in range(2, 6):
    months_set &= set(filtered_ratings[rating].index.astype(str))
common_months = sorted(months_set)

# Calculate total reports for each month across all ratings
total_reports_monthly = pd.DataFrame(index=common_months, columns=['Total Reports'])

for month in common_months:
    total_reports_monthly.loc[month, 'Total Reports'] = sum(filtered_ratings[rating].loc[month].sum() for rating in range(1, 6))

# Calculate percentage of each rating
percentages = {rating: (total_reports[rating] / total_reports_all) * 100 for rating in range(1, 6)}

# Plotting
plt.figure(figsize=(16, 8))

colors = ['blue', 'green', 'orange', 'purple', 'red']
labels = ['Rating 1', 'Rating 2', 'Rating 3', 'Rating 4', 'Rating 5']

# Combine data for plotting
combined_data = pd.DataFrame(index=common_months)
for rating in range(1, 6):
    combined_data[f'Rating {rating}'] = filtered_ratings[rating].loc[common_months, rating]

# Plot combined data
combined_data.plot(kind='bar', stacked=True, color=colors, width=0.8, ax=plt.gca())
plt.xlabel('Month')
plt.ylabel('Number of Reports')
plt.title('Number of Reports for Each Rating per Month')
plt.xticks(rotation=45)
plt.legend(labels, title='Ratings', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make room for the text above the title
plt.show()

# Calculate the counts and percentages of each rating
rating_selected = selected_data['user_rating'].value_counts().sort_index()
total_selected = rating_selected.sum()
rating_percentages_selected = (rating_selected / total_selected) * 100

# Print the counts and percentages
print("\nCounts of each rating for data without 'user_review':")
print(rating_selected)
print("\nPercentages of each rating for data without 'user_review':")
print(rating_percentages_selected)
print("\nTotal number of reports for data without 'user_review':")
total_selected

#REMOVE DATE FOR USER REVIEW ANALYSIS
# Remove specified columns
final_data = selected_data.drop(columns='date')
# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
final_data

# Tokenize the 'user_review' column
final_data['tokenized_review'] = final_data['user_review'].astype(str).apply(word_tokenize)

# Convert to lowercase
final_data['tokenized_review'] = final_data['tokenized_review'].apply(lambda tokens: [token.lower() for token in tokens])

# Convert tokenized reviews back to sentences
final_data['processed_review'] = final_data['tokenized_review'].apply(lambda tokens: ' '.join(tokens))

# Replace the 'user_review' column with 'processed_review'
final_data['user_review'] = final_data['processed_review']

# Drop the 'tokenized_review' and 'processed_review' columns if they are no longer needed
final_data.drop(columns=['tokenized_review', 'processed_review'], inplace=True)

# Display the updated DataFrame
print("Updated DataFrame:")
final_data.head()

"""#Language Detection"""

# Define a function to detect the language of each comment
def detect_language(text):
    try:
        return detect(text)
    except:
        return 'unknown'

# Apply language detection to each comment in 'user_review'
final_data['language'] = final_data['user_review'].apply(detect_language)

# Split into English and Indonesian comments
english_final_data = final_data[final_data['language'] == 'en']
other_final_data= final_data[final_data['language'] != 'en']

print("\nIndonesian Comments:")
other_final_data

print("\nEnglish Comments:")
english_final_data

# Filter for user_rating = 5
high_indo = other_final_data[other_final_data['user_rating'] == 5]
high_eng = english_final_data[english_final_data['user_rating'] == 5]

# Filter for user_rating between 3 and 4
medium_indo = other_final_data[(other_final_data['user_rating'] >= 3) & (other_final_data['user_rating'] <= 4)]
medium_eng = english_final_data[(english_final_data['user_rating'] >= 3) & (english_final_data['user_rating'] <= 4)]

# Filter for user_rating between 1 and 2
low_indo = other_final_data[(other_final_data['user_rating'] >= 1) & (other_final_data['user_rating'] <= 2)]
low_eng = english_final_data[(english_final_data['user_rating'] >= 1) & (english_final_data['user_rating'] <= 2)]

"""#Model buat sentiment analysis"""

tokenizer_indo = AutoTokenizer.from_pretrained("w11wo/indonesian-roberta-base-sentiment-classifier")
model_indo = AutoModelForSequenceClassification.from_pretrained("w11wo/indonesian-roberta-base-sentiment-classifier")

tokenizer_eng = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")
model_eng = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")

"""#FOR HIGH RATING (user_rating = 5) INDO"""

print(high_indo.shape)

example1 = high_indo['user_review'].iloc[4]
print(example1)

encoded_text = tokenizer_indo(example1, return_tensors='pt')
output = model_indo(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_negative' : scores[2].max(),
    'roberta_neutral' : scores[1].max(),
    'roberta_positive' : scores[0].max()
}
print(scores_dict)

def polarity_scores_roberta(example1):
    encoded_text = tokenizer_indo(example1, return_tensors='pt')
    output = model_indo(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[2],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[0]
    }
    return scores_dict

res = {}
for i, row in tqdm(high_indo.iterrows(), total=len(high_indo)):
    try:
        text = row['user_review']
        myid = row['hd']
        roberta_result = polarity_scores_roberta(text)
        both = {**roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_high_indo = pd.DataFrame(res).T
results_high_indo = results_high_indo.reset_index().rename(columns={'index': 'Id'})
results_high_indo = results_high_indo.merge(high_indo, how='left', left_on='Id', right_on='hd')

# Remove specified columns
columns_to_drop = ['Id','hd']
results_high_indo = results_high_indo.drop(columns=columns_to_drop)

# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
results_high_indo.head()

"""#FOR HIGH RATING(user_rating = 5) ENG"""

print(high_eng.shape)

example2 = high_eng['user_review'].iloc[1]
print(example2)

encoded_text = tokenizer_eng(example2, return_tensors='pt')
output = model_eng(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_negative' : scores[0].max(),
    'roberta_neutral' : scores[1].max(),
    'roberta_positive' : scores[2].max()
}
print(scores_dict)

def polarity_scores_roberta(example2):
    encoded_text = tokenizer_eng(example2, return_tensors='pt')
    output = model_eng(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

res = {}
for i, row in tqdm(high_eng.iterrows(), total=len(high_eng)):
    try:
        text = row['user_review']
        myid = row['hd']
        roberta_result = polarity_scores_roberta(text)
        both = {**roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_high_eng = pd.DataFrame(res).T
results_high_eng = results_high_eng.reset_index().rename(columns={'index': 'Id'})
results_high_eng = results_high_eng.merge(high_eng, how='left', left_on='Id', right_on='hd')

# Remove specified columns
columns_to_drop = ['Id','hd']
results_high_eng = results_high_eng.drop(columns=columns_to_drop)

# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
results_high_eng.head()

"""#FOR MEDIUM RATING (user_rating = 3-4) INDO"""

print(medium_indo.shape)

example3 = medium_indo['user_review'].iloc[1]
print(example3)

encoded_text = tokenizer_indo(example3, return_tensors='pt')
output = model_indo(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_negative' : scores[2].max(),
    'roberta_neutral' : scores[1].max(),
    'roberta_positive' : scores[0].max()
}
print(scores_dict)

def polarity_scores_roberta(example3):
    encoded_text = tokenizer_indo(example3, return_tensors='pt')
    output = model_indo(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[2],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[0]
    }
    return scores_dict

res = {}
for i, row in tqdm(medium_indo.iterrows(), total=len(medium_indo)):
    try:
        text = row['user_review']
        myid = row['hd']
        roberta_result = polarity_scores_roberta(text)
        both = {**roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_medium_indo = pd.DataFrame(res).T
results_medium_indo = results_medium_indo.reset_index().rename(columns={'index': 'Id'})
results_medium_indo = results_medium_indo.merge(medium_indo, how='left', left_on='Id', right_on='hd')
results_medium_indo

# Remove specified columns
columns_to_drop = ['Id','hd']
results_medium_indo = results_medium_indo.drop(columns=columns_to_drop)

# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
results_medium_indo

"""#FOR MEDIUM RATING (user_rating = 3-4) ENG"""

print(medium_eng.shape)

example4 = medium_eng['user_review'].iloc[5]
print(example4)

encoded_text = tokenizer_eng(example4, return_tensors='pt')
output = model_eng(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_negative' : scores[0].max(),
    'roberta_neutral' : scores[1].max(),
    'roberta_positive' : scores[2].max()
}
print(scores_dict)

def polarity_scores_roberta(example4):
    encoded_text = tokenizer_eng(example4, return_tensors='pt')
    output = model_eng(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

res = {}
for i, row in tqdm(medium_eng.iterrows(), total=len(medium_eng)):
    try:
        text = row['user_review']
        myid = row['hd']
        roberta_result = polarity_scores_roberta(text)
        both = {**roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_medium_eng = pd.DataFrame(res).T
results_medium_eng = results_medium_eng.reset_index().rename(columns={'index': 'Id'})
results_medium_eng = results_medium_eng.merge(medium_eng, how='left', left_on='Id', right_on='hd')

# Remove specified columns
columns_to_drop = ['Id','hd']
results_medium_eng = results_medium_eng.drop(columns=columns_to_drop)

# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
results_medium_eng.head()

"""#FOR LOW RATING (user_rating = 1-2) INDO"""

print(low_indo.shape)

example5 = low_indo['user_review'].iloc[5]
print(example5)

encoded_text = tokenizer_indo(example5, return_tensors='pt')
output = model_indo(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_negative' : scores[2].max(),
    'roberta_neutral' : scores[1].max(),
    'roberta_positive' : scores[0].max()
}
print(scores_dict)

def polarity_scores_roberta(example5):
    encoded_text = tokenizer_indo(example5, return_tensors='pt')
    output = model_indo(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[2],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[0]
    }
    return scores_dict

res = {}
for i, row in tqdm(low_indo.iterrows(), total=len(low_indo)):
    try:
        text = row['user_review']
        myid = row['hd']
        roberta_result = polarity_scores_roberta(text)
        both = {**roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_low_indo = pd.DataFrame(res).T
results_low_indo = results_low_indo.reset_index().rename(columns={'index': 'Id'})
results_low_indo = results_low_indo.merge(low_indo, how='left', left_on='Id', right_on='hd')

# Remove specified columns
columns_to_drop = ['Id','hd']
results_low_indo = results_low_indo.drop(columns=columns_to_drop)

# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
results_low_indo.head()

"""#FOR LOW RATING (user_rating = 1-2) ENG"""

print(low_eng.shape)

example6 = low_eng['user_review'].iloc[4]
print(example6)

encoded_text = tokenizer_eng(example6, return_tensors='pt')
output = model_eng(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_negative' : scores[0].max(),
    'roberta_neutral' : scores[1].max(),
    'roberta_positive' : scores[2].max()
}
print(scores_dict)

def polarity_scores_roberta(example6):
    encoded_text = tokenizer_eng(example6, return_tensors='pt')
    output = model_eng(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

res = {}
for i, row in tqdm(low_eng.iterrows(), total=len(low_eng)):
    try:
        text = row['user_review']
        myid = row['hd']
        roberta_result = polarity_scores_roberta(text)
        both = {**roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_low_eng = pd.DataFrame(res).T
results_low_eng = results_low_eng.reset_index().rename(columns={'index': 'Id'})
results_low_eng = results_low_eng.merge(low_eng, how='left', left_on='Id', right_on='hd')

# Remove specified columns
columns_to_drop = ['Id','hd']
results_low_eng = results_low_eng.drop(columns=columns_to_drop)

# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
results_low_eng.head()

"""#For True positive, False positive, True Negative and False Negative

**Postive 5-Star**
"""

results_high_indo.query('user_rating == 5') \
    .sort_values('roberta_pos', ascending=False)['user_review'].values[0]

results_high_eng.query('user_rating == 5') \
    .sort_values('roberta_pos', ascending=False)['user_review'].values[0]

"""**Negative 5-Star**"""

results_high_indo.query('user_rating == 5') \
    .sort_values('roberta_neg', ascending=False)['user_review'].values[1]

results_high_eng.query('user_rating == 5') \
    .sort_values('roberta_neg', ascending=False)['user_review'].values[1]

"""**Positive 1-Star**"""

results_low_indo.query('user_rating == 1') \
    .sort_values('roberta_pos', ascending=False)['user_review'].values[0]

results_low_eng.query('user_rating == 1') \
    .sort_values('roberta_pos', ascending=False)['user_review'].values[1]

"""#Combine Indonesian user reviews and Combine Engish user reviews into 2 dataframes"""

# Combine the dataframes
combined_indo = pd.concat([results_high_indo, results_medium_indo, results_low_indo])

# Sort by user_rating in ascending order
combined_indo_sorted = combined_indo.sort_values(by='user_rating', ascending=True)

# Display the combined and sorted DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
combined_indo_sorted

# Combine the dataframes
combined_eng = pd.concat([results_high_eng, results_medium_eng, results_low_eng])

# Sort by user_rating in ascending order
combined_eng_sorted = combined_eng.sort_values(by='user_rating', ascending=True)

# Display the combined and sorted DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
combined_eng_sorted

combined_indo_sorted2 = combined_indo_sorted

combined_eng_sorted2 = combined_eng_sorted

"""#WordCloud Indo"""

from google.colab import files
uploaded = files.upload()

#Tokenize the 'user_review' column
combined_indo_sorted2['tokenized_review'] = combined_indo_sorted2['user_review'].astype(str).apply(word_tokenize)

# Load the stopword list
stopword_list = []
with open("tala-stopwords-indonesia.txt", "r") as f:
    for line in f:
        stripped_line = line.strip()
        line_list = stripped_line.split()
        stopword_list.append(line_list[0])

# Function to remove stopwords
def remove_stopwords(tokens, stopwords):
    return [token for token in tokens if token not in stopwords]

# Add additional words to remove
additional_stopwords = ['kompas', 'epaper', 'koran', 'kompasid', 'aplikasi','berita','good','nya','yg','nice','app','aja','ok']
stopword_list.extend(additional_stopwords)

# Apply the stopword removal
combined_indo_sorted2['filtered_review'] = combined_indo_sorted2['tokenized_review'].apply(lambda tokens: remove_stopwords(tokens, stopword_list))

# Drop the 'tokenized_review' column
combined_indo_sorted.drop(columns=['tokenized_review'], inplace=True)

# Function to split a list of words into phrases of up to max_length words
def split_into_phrases(words, max_length=4):
    phrases = []
    for i in range(len(words)):
        for j in range(1, max_length + 1):
            if i + j <= len(words):
                phrases.append(' '.join(words[i:i + j]))
    return phrases

# Apply the split_into_phrases function to create a new column for phrases
combined_indo_sorted2['phrases'] = combined_indo_sorted2['filtered_review'].apply(split_into_phrases)

# Function to remove duplicate phrases within each review
def remove_duplicate_phrases(phrases):
    unique_phrases = list(set(phrases))  # Using set to remove duplicates
    return unique_phrases

# Apply the remove_duplicate_phrases function
combined_indo_sorted2['phrases'] = combined_indo_sorted2['phrases'].apply(remove_duplicate_phrases)

# Function to join phrases into a single string
def join_phrases(phrases):
    return ' '.join(phrases)

# Apply the join_phrases function to create a column for joined phrases
combined_indo_sorted2['joined_phrases'] = combined_indo_sorted2['phrases'].apply(join_phrases)

# Display the updated DataFrame
combined_indo_sorted2.head()

# Filter the DataFrame for roberta_pos >= 0.8
pos_reviews_indo = combined_indo_sorted2[combined_indo_sorted2['roberta_pos'] >= 0.8]

# Combine all filtered reviews into a single string
all_pos_reviews_indo = ' '.join([' '.join(review) for review in pos_reviews_indo['phrases']])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_pos_reviews_indo)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Filtered Positive Reviews (roberta_pos >= 0.8)')
plt.show()

# Combine all phrases into a single list and filter out phrases containing a comma
all_phrases = [phrase for phrases in pos_reviews_indo['phrases'] for phrase in phrases if ',' not in phrase]

# Count the frequency of each phrase
phrase_freq = Counter(all_phrases)

# Display the top 5 phrases with their frequencies
top_phrases = phrase_freq.most_common(5)
print("Top 5 Words Ind Positive:")
for phrase, freq in top_phrases:
    print(f"{phrase}: {freq}")

# Filter the DataFrame for roberta_pos >= 0.8
neu_reviews_indo = combined_indo_sorted2[combined_indo_sorted2['roberta_neu'] >= 0.8]

# Combine all filtered reviews into a single string
all_neu_reviews_indo = ' '.join([' '.join(review) for review in neu_reviews_indo['phrases']])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_neu_reviews_indo)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Filtered Neutral Reviews (roberta_neu >= 0.8)')
plt.show()

# Combine all phrases into a single list and filter out phrases containing a comma
all_phrases = [phrase for phrases in neu_reviews_indo['phrases'] for phrase in phrases if ',' not in phrase]

# Count the frequency of each phrase
phrase_freq = Counter(all_phrases)

# Display the top 20 phrases with their frequencies
top_phrases = phrase_freq.most_common(5)
print("Top 5 Words Ind Neutral:")
for phrase, freq in top_phrases:
    print(f"{phrase}: {freq}")

# Filter the DataFrame for roberta_neg >= 0.8
neg_reviews_indo = combined_indo_sorted2[combined_indo_sorted2['roberta_neg'] >= 0.8]

# Combine all filtered reviews into a single string
all_neg_reviews_indo = ' '.join([' '.join(review) for review in neg_reviews_indo['phrases']])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_neg_reviews_indo)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Filtered Negative Reviews (roberta_neg >= 0.8)')
plt.show()

# Combine all phrases into a single list and filter out phrases containing a comma
all_phrases = [phrase for phrases in neg_reviews_indo['phrases'] for phrase in phrases if ',' not in phrase]

# Count the frequency of each phrase
phrase_freq = Counter(all_phrases)

# Display the top 20 phrases with their frequencies
top_phrases = phrase_freq.most_common(5)
print("Top 5 Words Ind negative:")
for phrase, freq in top_phrases:
    print(f"{phrase}: {freq}")

# Determine the predominant sentiment for each review
combined_indo_sorted2['predominant_sentiment'] = combined_indo_sorted2[['roberta_neg', 'roberta_neu', 'roberta_pos']].idxmax(axis=1)

# Count the number of each sentiment
sentiment_counts = combined_indo_sorted2['predominant_sentiment'].value_counts()

# Calculate the percentages
total_reviews = len(combined_indo_sorted2)
negative_percentage = (sentiment_counts.get('roberta_neg', 0) / total_reviews) * 100
neutral_percentage = (sentiment_counts.get('roberta_neu', 0) / total_reviews) * 100
positive_percentage = (sentiment_counts.get('roberta_pos', 0) / total_reviews) * 100

# Display the results
print(f"Total Reviews Indonesian: {total_reviews}")
print(f"Negative Reviews: {negative_percentage:.2f}%")
print(f"Neutral Reviews: {neutral_percentage:.2f}%")
print(f"Positive Reviews: {positive_percentage:.2f}%")

"""#WordCloud English"""

# Additional stopwords to add
additional_stopwords = ['kompas', 'epaper', 'newspaper', 'kompasid', 'app', 'berita', 'new','thank','apps','news']

# Tokenization, lowercase conversion, and stopword removal
stop_words = set(stopwords.words('english'))
stop_words.update(additional_stopwords)

def process_text(text):
    tokens = word_tokenize(text)  # Tokenize the text
    tokens_lower = [token.lower() for token in tokens]  # Convert to lowercase
    tokens_filtered = [token for token in tokens_lower if token not in stop_words]  # Remove stopwords
    return tokens_filtered

# Function to split a list of words into phrases of up to max_length words
def split_into_phrases(words, max_length=4):
    phrases = []
    for i in range(len(words)):
        for j in range(1, max_length + 1):
            if i + j <= len(words):
                phrases.append(' '.join(words[i:i + j]))
    return phrases

# Apply processing to user_review column in combined_eng_sorted
combined_eng_sorted2['filtered_review'] = combined_eng_sorted2['user_review'].apply(process_text)

# Apply the split_into_phrases function to create a new column for phrases
combined_eng_sorted2['phrases'] = combined_eng_sorted2['filtered_review'].apply(split_into_phrases)

# Display the results
combined_eng_sorted2

# Filter the DataFrame for roberta_pos >= 0.8
pos_reviews_eng = combined_eng_sorted2[combined_eng_sorted2['roberta_pos'] >= 0.8]

# Combine all filtered reviews into a single string
all_pos_reviews_eng = ' '.join([' '.join(review) for review in pos_reviews_eng['phrases']])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_pos_reviews_eng)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Filtered Positive Reviews (roberta_pos >= 0.8)')
plt.show()

# Combine all phrases into a single list and filter out phrases containing a comma
all_phrases = [phrase for phrases in pos_reviews_eng['phrases'] for phrase in phrases if ',' not in phrase]

# Count the frequency of each phrase
phrase_freq = Counter(all_phrases)

# Display the top 20 phrases with their frequencies
top_phrases = phrase_freq.most_common(5)
print("Top 5 Words Eng Positive:")
for phrase, freq in top_phrases:
    print(f"{phrase}: {freq}")

# Filter the DataFrame for roberta_neu >= 0.8
neu_reviews_eng = combined_eng_sorted2[combined_eng_sorted2['roberta_neu'] >= 0.5]

# Combine all filtered reviews into a single string
all_neu_reviews_eng = ' '.join([' '.join(review) for review in neu_reviews_eng['phrases']])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_neu_reviews_eng)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Filtered Neutral Reviews (roberta_neu >= 0.8)')
plt.show()

# Combine all phrases into a single list and filter out phrases containing a comma
all_phrases = [phrase for phrases in neu_reviews_eng['phrases'] for phrase in phrases if ',' not in phrase]

# Count the frequency of each phrase
phrase_freq = Counter(all_phrases)

# Display the top 20 phrases with their frequencies
top_phrases = phrase_freq.most_common(5)
print("Top 5 Words Eng Neutral:")
for phrase, freq in top_phrases:
    print(f"{phrase}: {freq}")

# Filter the DataFrame for roberta_neg >= 0.8
neg_reviews_eng = combined_eng_sorted2[combined_eng_sorted2['roberta_neg'] >= 0.8]

# Combine all filtered reviews into a single string
all_neg_reviews_eng = ' '.join([' '.join(review) for review in neg_reviews_eng['phrases']])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_neg_reviews_eng)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Filtered Negative Reviews (roberta_neg >= 0.8)')
plt.show()

# Combine all phrases into a single list and filter out phrases containing a comma
all_phrases = [phrase for phrases in neg_reviews_eng['phrases'] for phrase in phrases if ',' not in phrase]

# Count the frequency of each phrase
phrase_freq = Counter(all_phrases)

# Display the top 20 phrases with their frequencies
top_phrases = phrase_freq.most_common(5)
print("Top 5 Words Eng Negative:")
for phrase, freq in top_phrases:
    print(f"{phrase}: {freq}")

# Determine the predominant sentiment for each review
combined_eng_sorted2['predominant_sentiment'] = combined_eng_sorted2[['roberta_neg', 'roberta_neu', 'roberta_pos']].idxmax(axis=1)

# Count the number of each sentiment
sentiment_counts = combined_eng_sorted2['predominant_sentiment'].value_counts()

# Calculate the percentages
total_reviews = len(combined_eng_sorted2)
negative_percentage = (sentiment_counts.get('roberta_neg', 0) / total_reviews) * 100
neutral_percentage = (sentiment_counts.get('roberta_neu', 0) / total_reviews) * 100
positive_percentage = (sentiment_counts.get('roberta_pos', 0) / total_reviews) * 100

# Display the results
print(f"Total Reviews English: {total_reviews}")
print(f"Negative Reviews: {negative_percentage:.2f}%")
print(f"Neutral Reviews: {neutral_percentage:.2f}%")
print(f"Positive Reviews: {positive_percentage:.2f}%")

"""#Model Training"""

# Function to label sentiment based on roberta_neg, roberta_neu, roberta_pos values
def label_sentiment(row):
    if row['roberta_pos'] > row['roberta_neg'] and row['roberta_pos'] > row['roberta_neu']:
        return 'positive'
    elif row['roberta_neg'] > row['roberta_pos'] and row['roberta_neg'] > row['roberta_neu']:
        return 'negative'
    else:
        return 'neutral'

# Apply sentiment labeling to both dataframes
combined_eng_sorted['sentiment'] = combined_eng_sorted.apply(label_sentiment, axis=1)
combined_indo_sorted['sentiment'] = combined_indo_sorted.apply(label_sentiment, axis=1)

# Combine dataframes for unified processing
combined_data = pd.concat([combined_eng_sorted, combined_indo_sorted], ignore_index=True)

# Split the combined data
X = combined_data['user_review']
y = combined_data['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)
# Train and evaluate models
models = {
    'Logistic Regression': LogisticRegression(),
    'SVM': SVC(),
    'Naive Bayes': MultinomialNB()
}

# Model training and evaluation
for model_name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    print(f"### Model: {model_name} ###")
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    print(classification_report(y_test, y_pred))

# Initialize sentiment analysis pipelines
pipeline_indo = pipeline("sentiment-analysis", model=model_indo, tokenizer=tokenizer_indo)
pipeline_eng = pipeline("sentiment-analysis", model=model_eng, tokenizer=tokenizer_eng)

# Function to apply the correct sentiment analysis based on language
def get_sentiment(review, language):
    if language == 'en':
        result = pipeline_eng(review)[0]
    else:
        result = pipeline_indo(review)[0]

    label = result['label'].lower()
    if 'positive' in label:
        return 'positive'
    elif 'negative' in label:
        return 'negative'
    else:
        return 'neutral'

# Apply sentiment analysis to combined_eng_sorted
combined_eng_sorted['predicted_sentiment'] = combined_eng_sorted.apply(lambda x: get_sentiment(x['user_review'], x['language']), axis=1)

# Apply sentiment analysis to combined_indo_sorted
combined_indo_sorted['predicted_sentiment'] = combined_indo_sorted.apply(lambda x: get_sentiment(x['user_review'], x['language']), axis=1)

# Combine dataframes for unified processing
combined_data = pd.concat([combined_eng_sorted, combined_indo_sorted], ignore_index=True)

# Balancing the dataset by upsampling the minority class
def balance_dataset(df):
    df_positive = df[df['sentiment'] == 'positive']
    df_neutral = df[df['sentiment'] == 'neutral']
    df_negative = df[df['sentiment'] == 'negative']

    max_count = max(len(df_positive), len(df_neutral), len(df_negative))

    df_positive_upsampled = resample(df_positive, replace=True, n_samples=max_count, random_state=42)
    df_neutral_upsampled = resample(df_neutral, replace=True, n_samples=max_count, random_state=42)
    df_negative_upsampled = resample(df_negative, replace=True, n_samples=max_count, random_state=42)

    return pd.concat([df_positive_upsampled, df_neutral_upsampled, df_negative_upsampled])

# Apply balancing
balanced_data = balance_dataset(combined_data)

# Split the balanced data
X = balanced_data['user_review']
y = balanced_data['sentiment']
languages = balanced_data['language']

X_train, X_test, y_train, y_test, languages_train, languages_test = train_test_split(X, y, languages, test_size=0.2, random_state=42)

# Evaluate the performance
def evaluate_pipeline(X_test, y_test, languages_test):
    y_pred = [get_sentiment(review, lang) for review, lang in zip(X_test, languages_test)]
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    print(classification_report(y_test, y_pred))

# Evaluate balanced data
print(f"### Evaluation for Balanced Data ###")
evaluate_pipeline(X_test, y_test, languages_test)

# Encode predicted sentiments to numeric labels
label_encoder = LabelEncoder()
balanced_data['encoded_sentiment'] = label_encoder.fit_transform(balanced_data['predicted_sentiment'])

# Split the balanced data
X = balanced_data['encoded_sentiment'].values.reshape(-1, 1)  # Use reshaped array for single feature
y = balanced_data['sentiment']
languages = balanced_data['language']

X_train, X_test, y_train, y_test, languages_train, languages_test = train_test_split(X, y, languages, test_size=0.2, random_state=42)

# Train and evaluate models
models = {
    'Logistic Regression': LogisticRegression(),
    'SVM': SVC(),
    'Naive Bayes': MultinomialNB()
}

for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"### Model: {model_name} ###")
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    print(classification_report(y_test, y_pred))

# Ensure X is a list of text and convert elements to strings
X = balanced_data['user_review'].astype(str)
y = balanced_data['sentiment']

# Split the balanced data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=5)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train Logistic Regression model
log_reg = LogisticRegression()
log_reg.fit(X_train_tfidf, y_train)

# Predict on the test set
y_pred_log_reg = log_reg.predict(X_test_tfidf)

# Evaluate performance
print("### Logistic Regression ###")
print(f"Accuracy: {accuracy_score(y_test, y_pred_log_reg)}")
print(classification_report(y_test, y_pred_log_reg))

# Define parameter grid for SVM
param_grid = {
    'C': [8, 10, 12],
    'gamma': [0.8, 1, 1.2],
    'kernel': ['rbf']
}

# Initialize SVM model
svm_model = SVC()

# Grid search cross-validation
grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)
grid_search.fit(X_train_tfidf, y_train)

# Print best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-validation Accuracy:", grid_search.best_score_)

# Initialize SVM model with best parameters
best_svm_model = SVC(C=grid_search.best_params_['C'], gamma=grid_search.best_params_['gamma'], kernel='rbf')

# Train the model on the entire training set
best_svm_model.fit(X_train_tfidf, y_train)

# Predict on the test set
y_pred_svm = best_svm_model.predict(X_test_tfidf)

# Evaluate performance
print("### SVM ###")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm)}")
print(classification_report(y_test, y_pred_svm))

# Function to predict sentiment using SVM model
def predict_sentiment_svm(review):
    # Process the review
    review_tfidf = vectorizer.transform([review])

    # Predict sentiment using SVM model
    prediction = best_svm_model.predict(review_tfidf)[0]
    return prediction

# Input review manually
user_review = input("Please enter a review: ")

# Predict sentiment
svm_prediction = predict_sentiment_svm(user_review)

# Print result
print(f"SVM Prediction for review: {user_review}")
print(f"Sentiment: {svm_prediction}")

# Function to get sentiment predictions for DataFrame
def get_svm_predictions(df):
    df['svm_predicted_sentiment'] = df['user_review'].apply(predict_sentiment_svm)
    return df

# Add SVM predictions to the DataFrame
combined_indo_sorted = get_svm_predictions(combined_indo_sorted)

# Save the DataFrame to CSV
output_file_path = "/content/combined_prediction_total.csv"
combined_indo_sorted.to_csv(output_file_path, index=False)

print(f"DataFrame saved as {output_file_path}")

from google.colab import files
uploaded = files.upload()

combined = pd.read_csv("combined.csv")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
# Assuming combined_indo_sorted has 'true_sentiment' and 'predicted_sentiment' columns
true_labels = combined['true_sentiment']
predicted_labels = combined['svm_predicted_sentiment']

# Compute confusion matrix
cm = confusion_matrix(true_labels, predicted_labels, labels=['negative', 'neutral', 'positive'])

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['negative', 'neutral', 'positive'])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Calculate sensitivity and specificity
def calculate_sensitivity_specificity(cm):
    # True positives, false positives, true negatives, false negatives for each class
    sensitivity = np.zeros(3)
    specificity = np.zeros(3)

    for i in range(3):
        TP = cm[i, i]
        FN = np.sum(cm[i, :]) - TP
        FP = np.sum(cm[:, i]) - TP
        TN = np.sum(cm) - (TP + FN + FP)

        sensitivity[i] = TP / (TP + FN) if (TP + FN) != 0 else 0
        specificity[i] = TN / (TN + FP) if (TN + FP) != 0 else 0

    return sensitivity, specificity

sensitivity, specificity = calculate_sensitivity_specificity(cm)

# Print sensitivity and specificity for each class
labels = ['negative', 'neutral', 'positive']
for i, label in enumerate(labels):
    print(f"{label.capitalize()} - Sensitivity (Recall): {sensitivity[i]:.2f}, Specificity: {specificity[i]:.2f}")

