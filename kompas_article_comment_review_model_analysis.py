# -*- coding: utf-8 -*-
"""Kompas_Article_Comment_Review_Model_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yCWp8AbcDWQQjmirHlhyInjkVSUW6sNw
"""

!pip install langdetect
!pip install transformers
!pip install scipy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import io
import re
import string
import nltk

from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.utils import shuffle
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from sklearn.utils import resample
from sklearn.preprocessing import LabelEncoder
from transformers import pipeline

from langdetect import detect
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax
from transformers import AutoModelForMaskedLM

from wordcloud import WordCloud
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from tqdm import tqdm
from nltk import word_tokenize, pos_tag
from nltk.chunk import ne_chunk

nltk.download('stopwords')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

plt.style.use('ggplot')

from google.colab import files
uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

raw = pd.read_csv(io.StringIO(uploaded['d_article_comment.csv'].decode('utf-8')))

raw.head()

#RAW SHAPE PRINTING
print(raw.shape)

# Convert 'partitiondt' to datetime if it's not already
raw['partitiondt'] = pd.to_datetime(raw['partitiondt'])

# Group by 'partitiondt' and count the number of comments
comment_counts = raw.groupby('partitiondt').size().reset_index(name='total_comments')

# Calculate the total number of comments
total_comments = comment_counts['total_comments'].sum()

# Find the date with the most and least comments
most_commented_date = comment_counts.loc[comment_counts['total_comments'].idxmax()]
least_commented_date = comment_counts.loc[comment_counts['total_comments'].idxmin()]

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(comment_counts['partitiondt'], comment_counts['total_comments'], marker='o', linestyle='-')
plt.title(f'Total Number of Comments per Date\nTotal Comments: {total_comments}')
plt.xlabel('Date')
plt.ylabel('Total Comments')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

# Annotate the most and least commented dates
plt.annotate(f"Most: {most_commented_date['partitiondt'].date()}\n({most_commented_date['total_comments']} comments)",
             xy=(most_commented_date['partitiondt'], most_commented_date['total_comments']),
             xytext=(most_commented_date['partitiondt'], most_commented_date['total_comments'] + 10),
             arrowprops=dict(facecolor='black', shrink=0.05))

plt.annotate(f"Least: {least_commented_date['partitiondt'].date()}\n({least_commented_date['total_comments']} comments)",
             xy=(least_commented_date['partitiondt'], least_commented_date['total_comments']),
             xytext=(least_commented_date['partitiondt'], least_commented_date['total_comments'] + 10),
             arrowprops=dict(facecolor='black', shrink=0.05))

plt.show()

# Print the most and least commented dates and their counts
print(f"Most commented date: {most_commented_date['partitiondt'].date()} with {most_commented_date['total_comments']} comments")
print(f"Least commented date: {least_commented_date['partitiondt'].date()} with {least_commented_date['total_comments']} comments")

# Group by 'partitiondt' and count the number of comments
comment_counts = raw.groupby('partitiondt').size().reset_index(name='total_comments')

# Find the date with the most comments
most_commented_date = comment_counts.loc[comment_counts['total_comments'].idxmax()]

# Get the article_slug with the most comments on the most commented date
most_commented_articles = raw[raw['partitiondt'] == most_commented_date['partitiondt']]
most_commented_article_slug = most_commented_articles['article_slug'].value_counts().idxmax()

# Print the most commented date and the article_slug with the most comments
print(f"Most commented date: {most_commented_date['partitiondt'].date()} with {most_commented_date['total_comments']} comments")
print(f"Article_slug with the most comments on the most commented date: {most_commented_article_slug}")

# Group by 'article_slug' and count the number of comments for each slug
article_slug_counts = raw.groupby('article_slug').size().reset_index(name='comment_count')

# Filter the groups where the count is exactly one
single_comment_slugs = article_slug_counts[article_slug_counts['comment_count'] == 1]

# Count the number of unique article_slugs with exactly one comment
num_single_comment_slugs = single_comment_slugs['article_slug'].nunique()

print(f"Number of article_slugs with only one comment: {num_single_comment_slugs}")

#Select only necessary columns for analysis
selected_columns = ['comment_id', 'partitiondt', 'name', 'article_slug', 'published_status','comment', 'main_category_slug']
selected_data = raw[selected_columns]

# Display the selected data
selected_data

# Filter the selected_data DataFrame to include only rows with empty or null 'user_comment'
raw_data_no_comment = selected_data[selected_data['comment'].isna() | (selected_data['comment'] == '')]

print("\nCounts of comment when null:")
print(raw_data_no_comment)

# Group by comment and count occurrences
grouped = selected_data.groupby('comment').size().reset_index(name='counts')

# Identify comments that appear more than once
common_comments = grouped[grouped['counts'] > 1]['comment']

# Calculate number of duplicated comments (each duplicate counted)
number_of_duplicated_comments = (grouped[grouped['counts'] > 1]['counts'] - 1).sum()

# Calculate total removed comments
removed_comments = common_comments.shape[0] + number_of_duplicated_comments

# Filter out all rows with these common comments
filtered_data = selected_data[~selected_data['comment'].isin(common_comments)]

# Calculate the number of unique comments and actual rows after filtering
unique_comments_after = filtered_data['comment'].nunique()
actual_rows_after = len(filtered_data)

# Calculate expected number of rows after filtering
expected_rows = len(selected_data) - removed_comments

# Display the results
print(f"Total rows before filtering: {len(selected_data)}")
print(f"Duplicated comments:{removed_comments}")
print(f"Actual number of rows after filtering: {actual_rows_after}")

# Calculate counts of published_status in the entire selected_data
total_counts = selected_data.groupby('published_status').size()

# Calculate total counts
total_counts_all = total_counts.sum()

# Calculate percentages
percentage_true_total = (total_counts.get(True, 0) / total_counts_all) * 100 if total_counts_all > 0 else 0
percentage_false_total = (total_counts.get(False, 0) / total_counts_all) * 100 if total_counts_all > 0 else 0

# Display percentages
print(f"Percentage of comments with published_status True in selected_data: {percentage_true_total:.2f}%")
print(f"Percentage of comments with published_status False in selected_data: {percentage_false_total:.2f}%")

# Group by comment and count occurrences
grouped = selected_data.groupby('comment').size().reset_index(name='counts')

# Identify comments that appear more than once
common_comments = grouped[grouped['counts'] > 1]['comment']

# Filter original data to include only rows with common comments
common_data = selected_data[selected_data['comment'].isin(common_comments)]

# Calculate counts of duplicated comments based on published_status
duplicated_counts = common_data.groupby('published_status').size()

# Calculate total count of duplicated comments
total_duplicated = duplicated_counts.sum()

# Calculate percentages
percentage_true = (duplicated_counts.get(True, 0) / total_duplicated) * 100 if total_duplicated > 0 else 0
percentage_false = (duplicated_counts.get(False, 0) / total_duplicated) * 100 if total_duplicated > 0 else 0

# Display percentages
print(f"Percentage of duplicated comments with published_status True: {percentage_true:.2f}%")
print(f"Percentage of duplicated comments with published_status False: {percentage_false:.2f}%")

# Group by comment and count occurrences
grouped = selected_data.groupby('comment').size().reset_index(name='counts')

# Identify comments that appear more than once
common_comments = grouped[grouped['counts'] > 1]['comment']

# Filter original data to include only rows with common comments
common_data = selected_data[selected_data['comment'].isin(common_comments)]

# Filter data to include only rows with non-duplicated comments
non_duplicated_data = selected_data[~selected_data['comment'].isin(common_comments)]

# Calculate counts of non-duplicated comments based on published_status
non_duplicated_counts = non_duplicated_data.groupby('published_status').size()

# Calculate total count of non-duplicated comments
total_non_duplicated = non_duplicated_counts.sum()

# Calculate percentages
percentage_true_non_duplicated = (non_duplicated_counts.get(True, 0) / total_non_duplicated) * 100 if total_non_duplicated > 0 else 0
percentage_false_non_duplicated = (non_duplicated_counts.get(False, 0) / total_non_duplicated) * 100 if total_non_duplicated > 0 else 0

# Display percentages
print(f"Percentage of non-duplicated comments with published_status True: {percentage_true_non_duplicated:.2f}%")
print(f"Percentage of non-duplicated comments with published_status False: {percentage_false_non_duplicated:.2f}%")

# Split into two DataFrames based on 'published_status'
published_true_raw = selected_data[selected_data['published_status'] == True]
published_false_raw = selected_data[selected_data['published_status'] == False]

# Display the DataFrames
print("Published True DataFrame:")
published_true_raw

print("Published false DataFrame:")
published_false_raw

# Group by 'main_category_slug' and count the number of unique 'article_slug' in each category
category_counts = published_true_raw.groupby('main_category_slug')['article_slug'].nunique().reset_index(name='article_count')

# Sort the result in descending order based on the unique article count
category_counts = category_counts.sort_values(by='article_count', ascending=False)

# Select the top 5 categories
top_2_category_counts = category_counts.head(2)

# Display the result
print("Top 2 main_category_slug by unique article_slug (published_status = True):")
print(top_2_category_counts)

# Group by 'main_category_slug' and count the number of unique 'article_slug' in each category
category_counts = published_true_raw.groupby('main_category_slug')['article_slug'].nunique().reset_index(name='article_count')

# Sort the result in descending order based on the unique article count
category_counts = category_counts.sort_values(by='article_count', ascending=True)

# Select the top 5 categories
top_2_category_counts = category_counts.head(2)

# Display the result
print("Top 2 main_category_slug by unique article_slug (published_status = True):")
print(top_2_category_counts)

# Group by 'main_category_slug' and count the number of unique 'article_slug' in each category
category_counts = published_false_raw.groupby('main_category_slug')['article_slug'].nunique().reset_index(name='article_count')

# Sort the result in descending order based on the unique article count
category_counts = category_counts.sort_values(by='article_count', ascending=False)

# Select the top 5 categories
top_2_category_counts = category_counts.head(2)

# Display the result
print("Top 2 main_category_slug by unique article_slug (published_status = False):")
print(top_2_category_counts)

# Group by 'main_category_slug' and count the number of unique 'article_slug' in each category
category_counts = published_false_raw.groupby('main_category_slug')['article_slug'].nunique().reset_index(name='article_count')

# Sort the result in descending order based on the unique article count
category_counts = category_counts.sort_values(by='article_count', ascending=True)

# Select the top 5 categories
top_2_category_counts = category_counts.head(2)

# Display the result
print("Top 2 main_category_slug by unique article_slug (published_status = False):")
print(top_2_category_counts)

# Display the resulting dataframe
filtered_data

# Function to remove emojis from text
def remove_emojis(text):
    return re.sub(r'[^\w\s,]', '', text)

# Function to count emojis in text
def count_emojis(text):
    return len(re.findall(r'[^\w\s,]', text))

# Store the original DataFrame
original_data = filtered_data.copy()

# Count total emojis in the original data
original_emoji_count = original_data['comment'].apply(count_emojis).sum()

# Apply the function to the comment column using .loc
filtered_data.loc[:, 'comment'] = filtered_data['comment'].apply(remove_emojis)

# Count remaining emojis in the processed data
remaining_emoji_count = filtered_data['comment'].apply(count_emojis).sum()

# Calculate the number of removed emojis
removed_emoji_count = original_emoji_count - remaining_emoji_count

# Identify rows that will be removed
removed_values = original_data[original_data['comment'].str.strip() == '']['comment'].tolist()
removed_values += original_data.loc[original_data.index.difference(filtered_data.index), 'comment'].tolist()

# Remove empty rows
data = filtered_data[filtered_data['comment'].str.strip() != '']

# Show the updated DataFrame
print("Updated DataFrame:")
data

# Show the total number of removed emojis
print("\nTotal number of removed emojis:", removed_emoji_count)

# Initialize a dictionary to keep count of punctuation marks
punctuation_counts = {p: 0 for p in string.punctuation}

# Count and remove punctuation marks from the 'comment' column
def count_and_remove_punctuation(text):
    global punctuation_counts
    for p in string.punctuation:
        punctuation_counts[p] += text.count(p)
    return text.translate(str.maketrans('', '', string.punctuation))

# Apply function using .loc to avoid the SettingWithCopyWarning
data.loc[:, 'comment'] = data['comment'].apply(count_and_remove_punctuation)

# Display the counts of removed punctuation marks
print("Punctuation Marks Removed and Their Counts:")
for p, count in punctuation_counts.items():
    if count > 0:
        print(f"{p}: {count}")

data.head()

# Remove rows where the 'comment' column is empty or contains only whitespace/newline
data = data[~data['comment'].str.strip().eq('')]
data

# Function to convert text to lowercase
def convert_to_lowercase(text):
    return text.lower()

# Apply the functions in sequence
data.loc[:, 'comment'] = data['comment'].apply(convert_to_lowercase)

data.head()

# Group by 'partitiondt' and count the number of comments
comment_counts = data.groupby('partitiondt').size().reset_index(name='total_comments')

# Calculate the total number of comments
total_comments = comment_counts['total_comments'].sum()

# Find the date with the most and least comments
most_commented_date = comment_counts.loc[comment_counts['total_comments'].idxmax()]
least_commented_date = comment_counts.loc[comment_counts['total_comments'].idxmin()]

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(comment_counts['partitiondt'], comment_counts['total_comments'], marker='o', linestyle='-', color='blue')
plt.title(f'Total Number of Comments per Date\nTotal Comments: {total_comments}')
plt.xlabel('Date')
plt.ylabel('Total Comments')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

# Annotate the most and least commented dates
plt.annotate(f"Most: {most_commented_date['partitiondt'].date()}\n({most_commented_date['total_comments']} comments)",
             xy=(most_commented_date['partitiondt'], most_commented_date['total_comments']),
             xytext=(most_commented_date['partitiondt'], most_commented_date['total_comments'] + 10),
             arrowprops=dict(facecolor='black', shrink=0.05))

plt.annotate(f"Least: {least_commented_date['partitiondt'].date()}\n({least_commented_date['total_comments']} comments)",
             xy=(least_commented_date['partitiondt'], least_commented_date['total_comments']),
             xytext=(least_commented_date['partitiondt'], least_commented_date['total_comments'] + 10),
             arrowprops=dict(facecolor='black', shrink=0.05))

plt.show()

# Print the most and least commented dates and their counts
print(f"Most commented date: {most_commented_date['partitiondt'].date()} with {most_commented_date['total_comments']} comments")
print(f"Least commented date: {least_commented_date['partitiondt'].date()} with {least_commented_date['total_comments']} comments")

# Find the date with the highest number of comments
max_comments = comment_counts.loc[comment_counts['total_comments'].idxmax()]

# Print date with highest number of comments and the count
print(f"Date with highest number of comments: {max_comments['partitiondt']} with {max_comments['total_comments']} comments")

# Count the total number of unique article slugs
total_unique_slugs = data['article_slug'].nunique()

print(f"Total number of unique article slugs: {total_unique_slugs}")

# Count the number of comments per article slug
comment_counts = data['article_slug'].value_counts()

# Convert to DataFrame for easier manipulation and renaming the columns
comment_counts_df = comment_counts.reset_index()
comment_counts_df.columns = ['article_slug', 'comment_count']

# Display the ranked articles by comment count
comment_counts_df.sort_values(by='comment_count', ascending=False, inplace=True)
comment_counts_df

# Count the number of comments per article slug
name_counts = data['name'].value_counts()

# Convert to DataFrame for easier manipulation and renaming the columns
name_counts_df = name_counts.reset_index()
name_counts_df.columns = ['name', 'comment_count']

# Display the ranked articles by comment count
name_counts_df.sort_values(by='comment_count', ascending=False, inplace=True)
name_counts_df

# Define a function to detect the language of each comment
def detect_language(text):
    try:
        return detect(text)
    except:
        return 'unknown'

# Apply language detection to each comment in 'comment'
data.loc[:, 'language'] = data['comment'].apply(detect_language)

# Split into English and Indonesian comment
english_data = data[data['language'] == 'en']
other_data= data[data['language'] != 'en']

print("\nIndonesian Comments:")
other_data

print("\nEnglish Comments")

# Display the resulting DataFrame
data2 = other_data

# Split into two DataFrames based on 'published_status'
published_true = data2[data2['published_status'] == True]
published_false = data2[data2['published_status'] == False]

# Display the DataFrames
print("Published True DataFrame:")
published_true

print("\nPublished False DataFrame:")
published_false

tokenizer_indo = AutoTokenizer.from_pretrained("w11wo/indonesian-roberta-base-sentiment-classifier")
model_indo = AutoModelForSequenceClassification.from_pretrained("w11wo/indonesian-roberta-base-sentiment-classifier")

example = published_true['comment'].iloc[1]
print(example)

encoded_text = tokenizer_indo(example, return_tensors='pt')
output = model_indo(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_negative' : scores[2].max(),
    'roberta_neutral' : scores[1].max(),
    'roberta_positive' : scores[0].max()
}
print(scores_dict)

def polarity_scores_roberta(example):
    encoded_text = tokenizer_indo(example, return_tensors='pt')
    output = model_indo(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[2],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[0]
    }
    return scores_dict

res = {}
for i, row in tqdm(published_true.iterrows(), total=len(published_true)):
    try:
        text = row['comment']
        myid = row['comment_id']
        roberta_result = polarity_scores_roberta(text)
        both = {**roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_data2 = pd.DataFrame(res).T
results_data2= results_data2.reset_index().rename(columns={'index': 'comment'})
results_data2 = results_data2.merge(other_data, how='left', left_on='comment', right_on='comment_id')
results_data2

# Remove rows where 'comment_y' is NaN
results_data2 = results_data2.dropna(subset=['comment_y'])

# Remove rows where 'comment_y' is empty or contains only whitespace/newline
results_data2 = results_data2[~results_data2['comment_y'].str.strip().eq('')]

results_data2

# Remove specified columns
columns_to_drop = ['comment_x', 'comment_id']
results_data2 = results_data2.drop(columns=columns_to_drop)

# Rename the column 'comment_y' to 'comment'
results_data2 = results_data2.rename(columns={'comment_y': 'comment'})

# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
results_data2

# Determine the predominant sentiment for each review
results_data2['predominant_sentiment'] = results_data2[['roberta_neg', 'roberta_neu', 'roberta_pos']].idxmax(axis=1)

# Count the number of each sentiment
sentiment_counts = results_data2['predominant_sentiment'].value_counts()

# Calculate the percentages
total_reviews = len(results_data2)
negative_percentage = (sentiment_counts.get('roberta_neg', 0) / total_reviews) * 100
neutral_percentage = (sentiment_counts.get('roberta_neu', 0) / total_reviews) * 100
positive_percentage = (sentiment_counts.get('roberta_pos', 0) / total_reviews) * 100

# Display the results
print(f"Total Reviews published_true: {total_reviews}")
print(f"Negative Reviews: {negative_percentage:.2f}%")
print(f"Neutral Reviews: {neutral_percentage:.2f}%")
print(f"Positive Reviews: {positive_percentage:.2f}%")

res = {}
for i, row in tqdm(published_false.iterrows(), total=len(published_false)):
    try:
        text = row['comment']
        myid = row['comment_id']
        roberta_result = polarity_scores_roberta(text)
        both = {**roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_data3 = pd.DataFrame(res).T
results_data3= results_data3.reset_index().rename(columns={'index': 'comment'})
results_data3 = results_data3.merge(other_data, how='left', left_on='comment', right_on='comment_id')
results_data3

# Remove rows where 'comment_y' is NaN
results_data3 = results_data3.dropna(subset=['comment_y'])

# Remove rows where 'comment_y' is empty or contains only whitespace/newline
results_data3 = results_data3[~results_data3['comment_y'].str.strip().eq('')]

results_data3

# Remove specified columns
columns_to_drop = ['comment_x', 'comment_id']
results_data3 = results_data3.drop(columns=columns_to_drop)

# Rename the column 'comment_y' to 'comment'
results_data3 = results_data3.rename(columns={'comment_y': 'comment'})

# Display the modified DataFrame
pd.set_option('display.max_columns', None)  # To display all columns
results_data3

# Determine the predominant sentiment for each review
results_data3['predominant_sentiment'] = results_data3[['roberta_neg', 'roberta_neu', 'roberta_pos']].idxmax(axis=1)

# Count the number of each sentiment
sentiment_counts = results_data3['predominant_sentiment'].value_counts()

# Calculate the percentages
total_reviews = len(results_data3)
negative_percentage = (sentiment_counts.get('roberta_neg', 0) / total_reviews) * 100
neutral_percentage = (sentiment_counts.get('roberta_neu', 0) / total_reviews) * 100
positive_percentage = (sentiment_counts.get('roberta_pos', 0) / total_reviews) * 100

# Display the results
print(f"Total Reviews published_false: {total_reviews}")
print(f"Negative Reviews: {negative_percentage:.2f}%")
print(f"Neutral Reviews: {neutral_percentage:.2f}%")
print(f"Positive Reviews: {positive_percentage:.2f}%")

from google.colab import files
uploaded = files.upload()

results_data2edit = results_data2
results_data3edit = results_data3

# Load stopwords from the file
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        stopwords = set(file.read().splitlines())
    return stopwords

stopwords = load_stopwords('tala-stopwords-indonesia.txt')

# Filter comments where roberta_neu is >= 0.8
filtered_comments = results_data2edit[results_data2edit['roberta_pos'] >= 0.8]['comment'].dropna()

# Define a function to tokenize comments into phrases (n-grams)
def tokenize_to_phrases(text, n):
    # Replace newlines and non-word characters with spaces, and convert to lower case
    text = re.sub(r'[\W_]+', ' ', text.lower())
    words = text.split()
    # Create n-grams (phrases)
    phrases = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
    return phrases

# Tokenize all comments into 2-word phrases (bigrams) and 3-word phrases (trigrams)
bigrams = []
trigrams = []

for comment in filtered_comments:
    bigrams.extend(tokenize_to_phrases(comment, 2))
    trigrams.extend(tokenize_to_phrases(comment, 3))

# Remove stopwords from bigrams and trigrams
def remove_stopwords(phrases, stopwords):
    filtered_phrases = []
    for phrase in phrases:
        words = phrase.split()
        if not any(word in stopwords for word in words):
            filtered_phrases.append(phrase)
    return filtered_phrases

bigrams = remove_stopwords(bigrams, stopwords)
trigrams = remove_stopwords(trigrams, stopwords)

# Count the frequency of each phrase type
bigram_freq = Counter(bigrams)
trigram_freq = Counter(trigrams)

# Display the top 10 most common bigrams and trigrams
top_bigrams = bigram_freq.most_common(5)
top_trigrams = trigram_freq.most_common(5)

print("Top 5 Common phrases:")
for phrase, freq in top_bigrams:
    print(f"{phrase}: {freq}")

print("\nTop 10 Common 3-word:")
for phrase, freq in top_trigrams:
    print(f"{phrase}: {freq}")

# Load stopwords from the file
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        stopwords = set(file.read().splitlines())
    return stopwords

stopwords = load_stopwords('tala-stopwords-indonesia.txt')

# Filter comments where roberta_pos is >= 0.8
filtered_comments = results_data2edit[results_data2edit['roberta_pos'] >= 0.8]['comment'].dropna()

# Define a function to tokenize comments into bigrams (2-word phrases)
def tokenize_to_bigrams(text):
    text = re.sub(r'[\W_]+', ' ', text.lower())
    words = text.split()
    bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]
    return bigrams

# Tokenize all comments into bigrams
bigrams = []
for comment in filtered_comments:
    bigrams.extend(tokenize_to_bigrams(comment))

# Count the frequency of each bigram, excluding stopwords
filtered_bigrams = [bigram for bigram in bigrams if not any(word in stopwords for word in bigram.split())]
bigram_freq = Counter(filtered_bigrams)

# Generate a word cloud from the bigram frequencies
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigram_freq)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Load stopwords from the file
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        stopwords = set(file.read().splitlines())
    return stopwords

stopwords = load_stopwords('tala-stopwords-indonesia.txt')

# Filter comments where roberta_neu is >= 0.8
filtered_comments = results_data2edit[results_data2edit['roberta_neu'] >= 0.8]['comment'].dropna()

# Define a function to tokenize comments into phrases (n-grams)
def tokenize_to_phrases(text, n):
    # Replace newlines and non-word characters with spaces, and convert to lower case
    text = re.sub(r'[\W_]+', ' ', text.lower())
    words = text.split()
    # Create n-grams (phrases)
    phrases = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
    return phrases

# Tokenize all comments into 2-word phrases (bigrams) and 3-word phrases (trigrams)
bigrams = []
trigrams = []

for comment in filtered_comments:
    bigrams.extend(tokenize_to_phrases(comment, 2))
    trigrams.extend(tokenize_to_phrases(comment, 3))

# Remove stopwords from bigrams and trigrams
def remove_stopwords(phrases, stopwords):
    filtered_phrases = []
    for phrase in phrases:
        words = phrase.split()
        if not any(word in stopwords for word in words):
            filtered_phrases.append(phrase)
    return filtered_phrases

bigrams = remove_stopwords(bigrams, stopwords)
trigrams = remove_stopwords(trigrams, stopwords)

# Count the frequency of each phrase type
bigram_freq = Counter(bigrams)
trigram_freq = Counter(trigrams)

# Display the top 10 most common bigrams and trigrams
top_bigrams = bigram_freq.most_common(10)
top_trigrams = trigram_freq.most_common(10)

print("Top 5 Common phrases:")
for phrase, freq in top_bigrams:
    print(f"{phrase}: {freq}")

print("\nTop 10 Common 3-word Phrases (Trigrams):")
for phrase, freq in top_trigrams:
    print(f"{phrase}: {freq}")

# Load stopwords from the file
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        stopwords = set(file.read().splitlines())
    return stopwords

stopwords = load_stopwords('tala-stopwords-indonesia.txt')

# Filter comments where roberta_pos is >= 0.8
filtered_comments = results_data2edit[results_data2edit['roberta_neu'] >= 0.8]['comment'].dropna()

# Define a function to tokenize comments into bigrams (2-word phrases)
def tokenize_to_bigrams(text):
    text = re.sub(r'[\W_]+', ' ', text.lower())
    words = text.split()
    bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]
    return bigrams

# Tokenize all comments into bigrams
bigrams = []
for comment in filtered_comments:
    bigrams.extend(tokenize_to_bigrams(comment))

# Count the frequency of each bigram, excluding stopwords
filtered_bigrams = [bigram for bigram in bigrams if not any(word in stopwords for word in bigram.split())]
bigram_freq = Counter(filtered_bigrams)

# Generate a word cloud from the bigram frequencies
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigram_freq)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Load stopwords from the file
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        stopwords = set(file.read().splitlines())
    return stopwords

stopwords = load_stopwords('tala-stopwords-indonesia.txt')

# Filter comments where roberta_neu is >= 0.8
filtered_comments = results_data2edit[results_data2edit['roberta_neg'] >= 0.8]['comment'].dropna()

# Define a function to tokenize comments into phrases (n-grams)
def tokenize_to_phrases(text, n):
    # Replace newlines and non-word characters with spaces, and convert to lower case
    text = re.sub(r'[\W_]+', ' ', text.lower())
    words = text.split()
    # Create n-grams (phrases)
    phrases = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
    return phrases

# Tokenize all comments into 2-word phrases (bigrams) and 3-word phrases (trigrams)
bigrams = []
trigrams = []

for comment in filtered_comments:
    bigrams.extend(tokenize_to_phrases(comment, 2))
    trigrams.extend(tokenize_to_phrases(comment, 3))

# Remove stopwords from bigrams and trigrams
def remove_stopwords(phrases, stopwords):
    filtered_phrases = []
    for phrase in phrases:
        words = phrase.split()
        if not any(word in stopwords for word in words):
            filtered_phrases.append(phrase)
    return filtered_phrases

bigrams = remove_stopwords(bigrams, stopwords)
trigrams = remove_stopwords(trigrams, stopwords)

# Count the frequency of each phrase type
bigram_freq = Counter(bigrams)
trigram_freq = Counter(trigrams)

# Display the top 10 most common bigrams and trigrams
top_bigrams = bigram_freq.most_common(10)
top_trigrams = trigram_freq.most_common(10)

print("Top 5 Common phrases:")
for phrase, freq in top_bigrams:
    print(f"{phrase}: {freq}")

print("\nTop 10 Common 3-word Phrases (Trigrams):")
for phrase, freq in top_trigrams:
    print(f"{phrase}: {freq}")

# Load stopwords from the file
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        stopwords = set(file.read().splitlines())
    return stopwords

stopwords = load_stopwords('tala-stopwords-indonesia.txt')

# Filter comments where roberta_pos is >= 0.8
filtered_comments = results_data2edit[results_data2edit['roberta_neg'] >= 0.8]['comment'].dropna()

# Define a function to tokenize comments into bigrams (2-word phrases)
def tokenize_to_bigrams(text):
    text = re.sub(r'[\W_]+', ' ', text.lower())
    words = text.split()
    bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]
    return bigrams

# Tokenize all comments into bigrams
bigrams = []
for comment in filtered_comments:
    bigrams.extend(tokenize_to_bigrams(comment))

# Count the frequency of each bigram, excluding stopwords
filtered_bigrams = [bigram for bigram in bigrams if not any(word in stopwords for word in bigram.split())]
bigram_freq = Counter(filtered_bigrams)

# Generate a word cloud from the bigram frequencies
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigram_freq)

# Display the word cloud
plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

#Select only necessary columns for modeling
selected_columns = ['comment_id', 'comment']
new_data = data[selected_columns]

# Display the selected data
new_data

"""#Modeling"""

# Combine the DataFrames vertically (stacked)
combined_data = pd.concat([results_data2, results_data3], ignore_index=True)

# Display the combined DataFrame
print("Combined DataFrame:")
combined_data

# Remove NaN values from the 'comment_y' column
combined_data = combined_data.dropna(subset=['comment'])

# Function to label sentiment based on roberta_neg, roberta_neu, roberta_pos values
def label_sentiment(row):
    if row['roberta_pos'] > row['roberta_neg'] and row['roberta_pos'] > row['roberta_neu']:
        return 'positive'
    elif row['roberta_neg'] > row['roberta_pos'] and row['roberta_neg'] > row['roberta_neu']:
        return 'negative'
    else:
        return 'neutral'

# Apply sentiment labeling to the DataFrame
combined_data['sentiment'] = combined_data.apply(label_sentiment, axis=1)

# Handle missing values in the 'comment' column by filling with empty strings
combined_data['comment'].fillna('', inplace=True)

# Split the combined data
X = combined_data['comment']
y = combined_data['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train and evaluate models
models = {
    'Logistic Regression': LogisticRegression(),
    'SVM': SVC(),
    'Naive Bayes': MultinomialNB()
}

# Model training and evaluation
for model_name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    print(f"### Model: {model_name} ###")
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    print(classification_report(y_test, y_pred))

from transformers import pipeline
from sklearn.utils import resample
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Initialize the sentiment analysis pipeline for Indonesian
pipeline_indo = pipeline("sentiment-analysis", model=model_indo, tokenizer=tokenizer_indo)

# Function to get sentiment
def get_sentiment(comment):
    result = pipeline_indo(comment)[0]
    label = result['label'].lower()
    if 'positive' in label:
        return 'positive'
    elif 'negative' in label:
        return 'negative'
    else:
        return 'neutral'

# Sample a subset of the data
subset_size = 5000  # Adjust this size as needed
sampled_data = combined_data.sample(n=subset_size, random_state=42)

# Apply sentiment analysis to the sampled data
sampled_data['predicted_sentiment'] = sampled_data['comment'].apply(get_sentiment)

# Split the data before balancing to ensure proper evaluation
X = sampled_data['comment']
y = sampled_data['predicted_sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Balancing the dataset by upsampling the minority class
def balance_dataset(df):
    df_positive = df[df['predicted_sentiment'] == 'positive']
    df_neutral = df[df['predicted_sentiment'] == 'neutral']
    df_negative = df[df['predicted_sentiment'] == 'negative']

    max_count = max(len(df_positive), len(df_neutral), len(df_negative))

    df_positive_upsampled = resample(df_positive, replace=True, n_samples=max_count, random_state=42)
    df_neutral_upsampled = resample(df_neutral, replace=True, n_samples=max_count, random_state=42)
    df_negative_upsampled = resample(df_negative, replace=True, n_samples=max_count, random_state=42)

    return pd.concat([df_positive_upsampled, df_neutral_upsampled, df_negative_upsampled])

# Apply balancing on the training data only
train_data = pd.DataFrame({'comment': X_train, 'predicted_sentiment': y_train})
balanced_train_data = balance_dataset(train_data)

# Split the balanced data
X_train_balanced = balanced_train_data['comment']
y_train_balanced = balanced_train_data['predicted_sentiment']

# TF-IDF Vectorization with bigrams
vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))
X_train_tfidf = vectorizer.fit_transform(X_train_balanced)
X_test_tfidf = vectorizer.transform(X_test)

# Define a classifier and perform hyperparameter tuning
classifier = LogisticRegression(max_iter=2000)

# Define parameter grid for GridSearchCV
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['newton-cg', 'lbfgs', 'liblinear']
}

# Perform GridSearchCV
grid_search = GridSearchCV(classifier, param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train_tfidf, y_train_balanced)

# Get the best classifier from grid search
best_classifier = grid_search.best_estimator_

# Predict and evaluate on the test set
y_pred = best_classifier.predict(X_test_tfidf)

# Evaluate the performance
print(f"### Best Parameters: {grid_search.best_params_} ###")
print(f"### Evaluation for Test Data ###")
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(classification_report(y_test, y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier

# Split the balanced data
X = balanced_train_data['comment']
y = balanced_train_data['predicted_sentiment']

# You may want to include a language column if applicable
# Assuming 'languages' column exists in 'balanced_train_data', otherwise remove the related code
# languages = balanced_train_data['language']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'SVM': SVC(),
    'Naive Bayes': MultinomialNB(),
    'Random Forest': RandomForestClassifier()
}

# Model training and evaluation
for model_name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    print(f"### Model: {model_name} ###")
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    print(classification_report(y_test, y_pred))

# Define the vectorizer
vectorizer = TfidfVectorizer()

# Define models and their parameter grids
models_param_grids = {
    'Logistic Regression': (LogisticRegression(), {
        'clf__C': [0.01, 0.1, 1, 10, 100],
        'clf__solver': ['newton-cg', 'lbfgs', 'liblinear']
    }),
    'SVM': (SVC(), {
        'clf__C': [0.01, 0.1, 1, 10, 100],
        'clf__kernel': ['linear', 'rbf']
    }),
    'Naive Bayes': (MultinomialNB(), {
        'clf__alpha': [0.1, 0.5, 1, 5, 10]
    }),
    'Random Forest': (RandomForestClassifier(), {
        'clf__n_estimators': [10, 50, 100, 200],
        'clf__max_depth': [None, 10, 20, 30]
    })
}

# Perform grid search for each model
best_models = {}
for model_name, (model, param_grid) in models_param_grids.items():
    pipeline = Pipeline([
        ('tfidf', vectorizer),
        ('clf', model)
    ])
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)
    grid_search.fit(X_train, y_train)
    best_models[model_name] = grid_search.best_estimator_
    print(f"### Best parameters for {model_name}: {grid_search.best_params_} ###")

# Evaluate the best models
for model_name, best_model in best_models.items():
    y_pred = best_model.predict(X_test)
    print(f"### Model: {model_name} ###")
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    print(classification_report(y_test, y_pred))

from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier, RandomForestClassifier

# Initialize the TF-IDF vectorizer with bigrams and more features
vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=15000)

# Vectorize the training data
X_train_vectorized = vectorizer.fit_transform(X_train_balanced)

# Apply SMOTE to the vectorized data
smote = SMOTE(random_state=42)
X_train_balanced_smote, y_train_balanced_smote = smote.fit_resample(X_train_vectorized, y_train_balanced)

# Define the Logistic Regression model and parameter grid
logistic_model = LogisticRegression(class_weight='balanced', max_iter=1000)

logistic_param_grid = {
    'clf__C': [0.01, 0.1, 1, 10, 100],
    'clf__solver': ['newton-cg', 'lbfgs', 'liblinear'],
}

# Create a pipeline with the Logistic Regression model
logistic_pipeline = Pipeline([
    ('clf', logistic_model)
])

# Use StratifiedKFold for better cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform Randomized Search with cross-validation
random_search = RandomizedSearchCV(logistic_pipeline, logistic_param_grid, cv=cv, n_jobs=-1, verbose=1, n_iter=30, random_state=42)
random_search.fit(X_train_balanced_smote, y_train_balanced_smote)

# Get the best model
best_logistic = random_search.best_estimator_
print(f"### Best parameters for Logistic Regression: {random_search.best_params_} ###")

# Vectorize the test data
X_test_vectorized = vectorizer.transform(X_test)

# Evaluate the best Logistic Regression model on the test set
y_pred_logistic = best_logistic.predict(X_test_vectorized)
print(f"### Model: Logistic Regression ###")
print(f"Accuracy: {accuracy_score(y_test, y_pred_logistic)}")
print(classification_report(y_test, y_pred_logistic))

# Stacking ensemble as an alternative approach
estimators = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('logistic', LogisticRegression(max_iter=1000, class_weight='balanced'))
]
stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(class_weight='balanced', max_iter=1000))
stacking_clf.fit(X_train_balanced_smote, y_train_balanced_smote)

# Evaluate Stacking Classifier
y_pred_stacking = stacking_clf.predict(X_test_vectorized)
print(f"### Model: Stacking Logistic Regression ###")
print(f"Accuracy: {accuracy_score(y_test, y_pred_stacking)}")
print(classification_report(y_test, y_pred_stacking))

# Function to predict sentiment of a single comment using the best logistic regression model
def predict_sentiment(comment):
    comment_tfidf = vectorizer.transform([comment])
    sentiment = best_logistic.predict(comment_tfidf)
    return sentiment[0]

# Allow manual input and predict sentiment
while True:
    comment = input("Enter a comment: ")
    if comment.lower() == 'exit':
        break
    predicted_sentiment = predict_sentiment(comment)
    print(f"Predicted Sentiment: {predicted_sentiment}")

"""#END"""

# Apply the predict_sentiment function to each comment in the DataFrame
combined_data['predicted_sentiment'] = combined_data['comment'].apply(predict_sentiment)

# Save the DataFrame with predictions to a CSV file
output_filename = "predicted_sentiments.csv"
combined_data.to_csv(output_filename, index=False)

print(f"Predictions saved to {output_filename}")

